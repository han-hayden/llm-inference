# Sangfor AICP 大模型推理性能测试工具 — 用户需求文档

> 版本：v2.0 | 日期：2026-02-23

---

## 1. 项目概述

### 1.1 项目背景
Sangfor AICP 平台在大模型推理优化过程中，需要一套可视化性能测试工具，用于**采集基线数据、分析性能瓶颈、对比优化效果**，帮助运维和开发人员量化优化成果。

### 1.2 目标用户
- AICP 平台运维工程师
- 模型推理优化工程师
- 技术支持人员

### 1.3 核心价值
- 一站式完成"基线采集 → 分析 → 优化压测 → 效果对比 → 报告导出"闭环
- 代理模式接入，对现有业务零侵入
- 可视化仪表盘直观展示优化成果
- 支持导出带图表的专业 PDF 性能测试报告

### 1.4 参考实现
- 基于 `llm-inference-forward`（OpenAI-Forward）项目的代理转发和指标采集方案
- 复用其 SSE 流式处理、性能指标提取、数据导出等核心能力

---

## 2. 用户场景（User Stories）

### US-01：部署与启动
> 作为运维工程师，我希望通过 Helm Chart 一键部署工具到 K8s 集群，并自定义服务 IP 和端口，以便快速投入使用。

**验收标准：**
- 支持通过 Helm Chart 部署到 K8s 集群
- 支持自定义 Service IP 和端口（前端端口 + 代理转发端口分离）
- 部署完成后可通过浏览器访问工具前端
- 支持简单的用户名密码登录

### US-02：接入模型服务
> 作为优化工程师，我希望在页面上配置待优化模型服务的地址，工具自动作为代理转发请求，不中断现有业务。

**验收标准：**
- 页面可配置目标模型服务的 IP + 端口
- 配置后，访问工具的代理地址可透明转发到目标模型服务
- 支持 OpenAI 兼容 API（`/v1/chat/completions` 等）
- 支持自定义 HTTP API 格式（用户可配置 tokens 字段的 JSON Path）
- 转发过程中业务请求不中断，无感知切换
- 支持强制开启 `stream=true` 和 `stream_options.include_usage=true` 以获取完整指标

### US-03：基线数据采集
> 作为优化工程师，我希望启动基线采集任务后，工具自动记录经过代理的真实请求数据和性能指标，以便建立性能基线。

**验收标准：**
- 支持设定采集条件：按时间（如1天）或按请求数量（如500条）
- 自动记录每次请求的性能指标：
  - 输入 tokens 长度（从 `usage.prompt_tokens` 提取，同时支持本地 tokenizer 计算校验）
  - 输出 tokens 长度（从 `usage.completion_tokens` 提取）
  - TTFT（首 token 时延：`first_token_time - request_arrival_time`，单位 ms）
  - KV Cache 命中 tokens 数（从 `usage.prompt_tokens_details.cached_tokens` 或 `num_cached_tokens` 提取）
  - TPOT（每输出一个 token 的耗时：`decode_total_time / (chunk_count - 1)`，单位 ms）
  - Decode 速度（TPS：`(chunk_count - 1) / decode_total_time`，tokens/s）
  - E2E 端到端时延（`completion_time - arrival_time`，单位 ms）
- 自动记录每次请求的完整输入 messages 和响应 content，整理为测试 QA 对
- 采集过程中不影响正常业务请求的响应
- **数据存储为文件**（CSV + JSON 格式），保存到后台固定路径，支持文件轮转（单文件最大1000条记录）

### US-04：基线分析展示
> 作为优化工程师，我希望采集结束后看到直观的可视化分析仪表盘，了解当前模型服务的性能特征。

**验收标准：**
- 展示应用数据特征分布图：
  - 上下文长度分布（输入 tokens 长度分布直方图）
  - 模型响应时延分布（E2E 时延分布直方图）
  - KV Cache 命中率分布
- 展示核心性能指标汇总：
  - TTFT 平均值、P50、P90、P99
  - TPOT 平均值、P50、P90、P99
  - Decode 速度（TPS）平均值、P50、P90、P99
  - E2E 时延平均值、P50、P90、P99
- 展示性能基线数据表格（可分页、排序、搜索）
- 展示 QA 对数据表格（可查看请求/响应详情）
- **前端通过 API 读取后台固定路径下的文件进行解析展示**
- **可查看每次历史测试记录**，以列表形式展示所有采集/压测任务

### US-05：优化后压测
> 作为优化工程师，我希望在模型优化后，使用之前记录的 QA 对进行流量回放压测，验证优化效果。

**验收标准：**
- 支持选择已采集的 QA 对数据集进行回放
- 支持按真实业务请求顺序回放（sequential 模式）
- 支持指定并发数进行压力测试（concurrent 模式）
- 支持使用外部数据集（上传 JSON/CSV 文件）指定并发进行测试
- 压测过程中实时展示进度（已完成/总数）
- 压测完成后自动记录同样的性能指标，存储为文件
- 支持配置请求间隔（delay）和超时时间

### US-06：优化效果对比
> 作为优化工程师，我希望选择两次测试记录进行对比，直观看到优化前后的性能提升幅度。

**验收标准：**
- 支持选择任意两次测试记录进行对比
- 关键指标卡片展示（每个指标一张卡片）：
  - **首字延迟优化**：对比两次测试中所有请求 TTFT 的平均值，展示下降百分比
  - **吞吐量优化**：对比两次测试中所有请求 decode 速度（TPS）的平均值，展示提升百分比
- 对比曲线图表：
  - **TTFT 对比图**：横轴为请求次序，纵轴为 TTFT 耗时，同一图中叠加两条曲线
  - **Decode 速度对比图**：横轴为请求次序，纵轴为 decode 速度，同一图中叠加两条曲线

### US-07：SSH 远程接入
> 作为运维工程师，我希望通过 SSH 连接到工具的后台终端，方便排查问题、查看日志、手动管理数据文件。

**验收标准：**
- 工具容器内运行 SSH 服务，支持通过 SSH 客户端连接
- 支持自定义 SSH 端口（默认 22，可通过 Helm values 配置）
- 使用用户名密码认证（与前端登录账户一致或独立配置）
- SSH 登录后可访问数据文件目录（`/data/results/`）
- 可查看后端服务日志、执行运维命令
- 支持 SCP/SFTP 传输文件（上传数据集、下载报告等）

### US-08：性能测试报告导出
> 作为优化工程师，我希望将测试结果导出为专业的 PDF 报告，包含图表和数据分析，方便汇报和归档。

**验收标准：**
- 支持导出带图表的 PDF 性能测试报告
- 报告内容包含：
  - **报告封面**：项目名称、测试时间范围、目标模型服务信息
  - **数据特征分析**：上下文长度分布图、时延分布图、cache命中分布图
  - **基线性能指标**：TTFT/TPOT/TPS/E2E 的统计表格和图表
  - **优化效果对比**（如有）：
    - 关键指标提升百分比汇总表
    - TTFT 优化前后对比曲线图
    - Decode 速度优化前后对比曲线图
  - **详细数据表格**：每次请求的详细性能数据（分页展示）
- 图表以图片形式嵌入 PDF，保证可读性
- 支持自定义报告标题和备注

### US-09：性能优化建议（扩展预留）
> 作为优化工程师，我希望工具能基于采集的数据自动分析性能瓶颈，给出针对性的优化建议，减少人工分析成本。

**说明：** 此功能为后续扩展，当前版本仅预留接口和 UI 占位，不实现具体分析逻辑。

**预期能力（后续实现）：**
- 基于采集数据自动识别性能瓶颈类型：
  - Prefill 瓶颈（TTFT 过高）：可能原因为输入过长、KV Cache 命中率低、显存不足等
  - Decode 瓶颈（TPOT 过高/TPS 过低）：可能原因为 batch size 不合理、量化精度问题等
  - KV Cache 效率问题：命中率低、内存分配策略不优等
  - 长尾延迟问题：P99 远高于 P50，可能存在调度或资源竞争
- 自动生成优化建议列表（如调整 batch size、启用 prefix caching、调整量化策略等）
- 优化建议仅在前端页面展示，**不纳入 PDF 报告**
- 支持对接外部分析引擎（通过插件/API 扩展）

**当前版本验收标准：**
- 后端预留 `AnalysisEngine` 抽象接口和插件注册机制
- 前端测试详情页预留"优化建议"面板（显示"即将推出"占位）
- API 预留 `GET /api/analysis/{task_id}/suggestions` 端点（当前返回空列表）

---

## 3. 非功能需求

### 3.1 性能要求
- 代理转发延迟增加不超过 5ms
- 前端页面加载时间不超过 3s
- 支持同时采集至少 1000 条请求数据
- 文件自动轮转，单文件最大 1000 条记录

### 3.2 可用性
- 代理模式下保证请求 100% 转发，不丢失
- 采集任务支持断点续采（异常恢复后继续）
- 历史测试记录持久化保存，重启后可查看

### 3.3 数据存储
- 测试记录以表格文件（CSV + JSON）形式保存在后台固定路径（如 `/data/results/`）
- 前端通过 API 获取文件列表和文件内容进行解析展示
- 每次测试任务生成独立的文件目录，目录名包含任务ID和时间戳
- 同时导出 CSV（详细数据）和 JSON（含统计摘要）两种格式

### 3.4 UI/UX 风格
- **深色主题**：深灰/藏蓝/纯黑背景
- **强调色**：极光绿、电光蓝、深紫渐变色
- **毛玻璃效果**：卡片和面板使用 Frosted Glass 风格
- **发光边框**：边框带细线性渐变或 Glow 效果
- **模块化卡片布局**：数据通过卡片承载
- **极简非衬线字体**：线条感强，搭配发光小图标
- **平滑动态曲线图表**：图表带动画过渡效果

---

### 3.5 扩展性
- 后端采用插件化架构，分析引擎通过抽象接口接入，支持后续替换/扩展
- API 设计遵循 RESTful 规范，方便后续与外部系统集成
- 前端页面模块化，分析建议面板可独立扩展

---

## 4. 约束条件
- 部署环境：K8s 集群（通过 Helm Chart）
- 需兼容 Sangfor AICP 平台环境
- 模型服务 API 需同时支持 OpenAI 兼容格式和自定义 HTTP 格式
- 参考 llm-inference-forward 项目的代理转发和指标采集方案
